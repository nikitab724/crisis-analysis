{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6720253d-70e2-42e6-92cb-a947825e1b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter \n",
    "import numpy as np\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d69fe73-8c3d-447f-b10d-87e773e5ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cf60be7d-c377-4fc7-bc62-7c96bc44d3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'disasters': ['Earthquake', 'Seismic Event', 'Tremor', 'Tsunami', 'Tidal Wave', 'Volcanic Eruption', 'Volcanic Explosion', 'Landslide', 'Rockslide', 'Mudslide', 'Avalanche', 'Snow Avalanche', 'Sinkhole', 'Ground Collapse', 'Hurricane', 'Cyclone', 'Typhoon', 'Superstorm', 'Tornado', 'Twister', 'Blizzard', 'Snowstorm', 'Ice Storm', 'Freezing Rain', 'Hailstorm', 'Hailstorm Event', 'Dust Storm', 'Sandstorm', 'Derecho', 'Windstorm', 'Flood', 'Severe Flooding', 'Flash Flood', 'Sudden Flood', 'Storm Surge', 'Coastal Flooding', 'Seiche', 'Lake Surge', 'Drought', 'Severe Drought', 'Heatwave', 'Extreme Heatwave', 'Wildfire', 'Forest Fire', 'Brush Fire', 'Firestorm', 'Cold Snap', 'Extreme Cold', 'Polar Vortex', 'Pandemic', 'Global Pandemic', 'Epidemic', 'Disease Outbreak', 'Insect Plague', 'Locust Swarm', 'Animal Stampede', 'Asteroid Impact', 'Meteor Strike', 'Solar Flare', 'Geomagnetic Storm', 'Gamma-Ray Burst', 'Nuclear Disaster', 'Radiation Leak', 'Nuclear Meltdown', 'Chemical Spill', 'Toxic Spill', 'Hazardous Waste Leak', 'Industrial Explosion', 'Factory Explosion', 'Plant Explosion', 'Dam Failure', 'Dam Collapse', 'Blackout', 'Power Grid Failure', 'Terrorist Attack', 'Bombing Attack', 'War', 'Military Conflict', 'Civil Unrest', 'Riots', 'Mass Shooting', 'Active Shooter Incident', 'Cyberattack', 'Cybersecurity Breach', 'Deforestation', 'Forest Degradation', 'Desertification', 'Soil Erosion', 'Environmental Degradation', 'Oil Spill', 'Massive Oil Spill', 'Radioactive Contamination', 'Radiation Exposure', 'Transportation Accident', 'Major Crash', 'Structural Collapse', 'Building Collapse', 'Mine Disaster', 'Underground Collapse', 'Gas Leak Explosion', 'Pipeline Explosion', 'Chemical Explosion']}\n"
     ]
    }
   ],
   "source": [
    "def load_data(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "d_types = load_data(\"../data/disaster_types.json\")\n",
    "print(d_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "423f2f6d-910f-488b-bccc-a99a1abefedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns added: 102\n",
      "Sample patterns: [{'label': 'DISASTER', 'pattern': 'Earthquake'}, {'label': 'DISASTER', 'pattern': 'Seismic Event'}, {'label': 'DISASTER', 'pattern': 'Tremor'}, {'label': 'DISASTER', 'pattern': 'Tsunami'}, {'label': 'DISASTER', 'pattern': 'Tidal Wave'}]\n"
     ]
    }
   ],
   "source": [
    "def create_training_data(file, type):\n",
    "    data = load_data(file)\n",
    "    disasters = data[\"disasters\"]\n",
    "    patterns = []\n",
    "    for item in disasters:\n",
    "        pattern = {\n",
    "            \"label\": type,\n",
    "            \"pattern\": item\n",
    "        } \n",
    "        patterns.append(pattern)\n",
    "    return patterns\n",
    "\n",
    "def generate_rules(patterns):\n",
    "    nlp = English()\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", config={\"overwrite_ents\": True})\n",
    "    ruler.add_patterns(patterns)\n",
    "\n",
    "    print(f\"Total patterns added: {len(ruler.patterns)}\")\n",
    "    print(f\"Sample patterns: {ruler.patterns[:5]}\")  # Print first 5 to verify\n",
    "    \n",
    "    nlp.to_disk(\"disaster_ner\")\n",
    "\n",
    "patterns = create_training_data(\"../data/disaster_types.json\", \"DISASTER\")\n",
    "generate_rules(patterns)\n",
    "#Rules already generated (check app folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "76f4d8f3-1ec8-4824-9ef1-cdf00ea65dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../data/california_wildfires_final_data.json\")\n",
    "df = df[[\"tweet_id\", \"tweet_text\"]]\n",
    "df1 = df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3f0c6a6e-cd87-45e1-b7ef-679ec5e3a802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entity_ruler']\n",
      "No entities detected\n"
     ]
    }
   ],
   "source": [
    "def test_model(text):\n",
    "    nlp = spacy.load(\"disaster_ner\")\n",
    "    print(nlp.pipe_names)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    if not doc.ents:\n",
    "        print(\"No entities detected\")\n",
    "    for ent in doc.ents:\n",
    "        print(\"Entities found:\")\n",
    "        print(ent.text, ent.label_)\n",
    "\n",
    "test_model(\"earthquake\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6467fe7-d754-421d-8689-da93944c7836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Gizmodo: Wildfires raging through Northern California are terrifying  \n",
      "['Northern California']\n",
      "PHOTOS: Deadly wildfires rage in California  \n",
      "['California']\n",
      "RT @Cal_OES: PLS SHARE: Were capturing wildfire response, recovery info here:  \n",
      "[]\n",
      "RT @Cal_OES: PLS SHARE: Were capturing wildfire response, recovery info here:  \n",
      "[]\n",
      "RT @TIME: California's raging wildfires as you've never seen them before  \n",
      "['California']\n",
      "Extracted locations:  {0: ['Northern California'], 1: ['California'], 2: [], 3: [], 4: ['California']}\n",
      "Location counts:  Counter({'California': 2, 'Northern California': 1})\n"
     ]
    }
   ],
   "source": [
    "def extract_locations(text):\n",
    "    doc = NER(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == \"GPE\" or ent.label_ == \"LOC\"]\n",
    "\n",
    "location_dict = {}\n",
    "for i in range(len(df1)):\n",
    "    locations = extract_locations(df1.loc[i, \"tweet_text\"])\n",
    "    location_dict[i] = locations\n",
    "    print(df1.loc[i, \"tweet_text\"])\n",
    "    print(location_dict[i])\n",
    "\n",
    "all_locations = [loc for locs in location_dict.values() for loc in locs]\n",
    "\n",
    "location_counts = Counter(all_locations)\n",
    "\n",
    "print(\"Extracted locations: \", location_dict)\n",
    "print(\"Location counts: \", location_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ac39c54-2c46-4ef8-af02-277cf144d2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> RT @Gizmodo, ORG \n",
      "\n",
      "0 -> Northern California, LOC \n",
      "\n",
      "1 -> California, GPE \n",
      "\n",
      "2 -> PLS SHARE, ORG \n",
      "\n",
      "3 -> PLS SHARE, ORG \n",
      "\n",
      "4 -> RT @TIME, PERSON \n",
      "\n",
      "4 -> California, GPE \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking all entities that were found\n",
    "def extract_all_entities(text):\n",
    "    doc = NER(text)\n",
    "    for ent in doc.ents:\n",
    "        print(f\"{i} -> {ent.text}, {ent.label_} \\n\")\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    extract_all_entities(df1.loc[i, \"tweet_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a7fa91-672f-4fc0-bf20-aa62325298a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
